{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 27159,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.055230310394344416,
      "grad_norm": 6.6031813621521,
      "learning_rate": 4.9081335837107405e-05,
      "loss": 0.7371,
      "step": 500
    },
    {
      "epoch": 0.11046062078868883,
      "grad_norm": 4.811368942260742,
      "learning_rate": 4.816083066386833e-05,
      "loss": 0.5359,
      "step": 1000
    },
    {
      "epoch": 0.16569093118303324,
      "grad_norm": 4.680866718292236,
      "learning_rate": 4.724032549062926e-05,
      "loss": 0.4881,
      "step": 1500
    },
    {
      "epoch": 0.22092124157737766,
      "grad_norm": 4.189964294433594,
      "learning_rate": 4.631982031739018e-05,
      "loss": 0.4667,
      "step": 2000
    },
    {
      "epoch": 0.2761515519717221,
      "grad_norm": 4.174421787261963,
      "learning_rate": 4.539931514415111e-05,
      "loss": 0.4458,
      "step": 2500
    },
    {
      "epoch": 0.3313818623660665,
      "grad_norm": 3.4795773029327393,
      "learning_rate": 4.447880997091204e-05,
      "loss": 0.4307,
      "step": 3000
    },
    {
      "epoch": 0.38661217276041093,
      "grad_norm": 3.493759870529175,
      "learning_rate": 4.3558304797672967e-05,
      "loss": 0.4298,
      "step": 3500
    },
    {
      "epoch": 0.44184248315475533,
      "grad_norm": 4.281454563140869,
      "learning_rate": 4.263779962443389e-05,
      "loss": 0.4201,
      "step": 4000
    },
    {
      "epoch": 0.4970727935490997,
      "grad_norm": 3.5109376907348633,
      "learning_rate": 4.171729445119482e-05,
      "loss": 0.4105,
      "step": 4500
    },
    {
      "epoch": 0.5523031039434442,
      "grad_norm": 4.414114475250244,
      "learning_rate": 4.0796789277955744e-05,
      "loss": 0.4001,
      "step": 5000
    },
    {
      "epoch": 0.6075334143377886,
      "grad_norm": 5.207272529602051,
      "learning_rate": 3.987628410471667e-05,
      "loss": 0.3998,
      "step": 5500
    },
    {
      "epoch": 0.662763724732133,
      "grad_norm": 3.1549229621887207,
      "learning_rate": 3.8955778931477596e-05,
      "loss": 0.3925,
      "step": 6000
    },
    {
      "epoch": 0.7179940351264774,
      "grad_norm": 3.677220344543457,
      "learning_rate": 3.803527375823852e-05,
      "loss": 0.3884,
      "step": 6500
    },
    {
      "epoch": 0.7732243455208219,
      "grad_norm": 3.792248249053955,
      "learning_rate": 3.711476858499945e-05,
      "loss": 0.3848,
      "step": 7000
    },
    {
      "epoch": 0.8284546559151662,
      "grad_norm": 4.787006378173828,
      "learning_rate": 3.6194263411760373e-05,
      "loss": 0.3777,
      "step": 7500
    },
    {
      "epoch": 0.8836849663095107,
      "grad_norm": 5.129867076873779,
      "learning_rate": 3.5273758238521306e-05,
      "loss": 0.3761,
      "step": 8000
    },
    {
      "epoch": 0.9389152767038551,
      "grad_norm": 4.133213043212891,
      "learning_rate": 3.4353253065282225e-05,
      "loss": 0.3682,
      "step": 8500
    },
    {
      "epoch": 0.9941455870981994,
      "grad_norm": 3.736915111541748,
      "learning_rate": 3.343274789204315e-05,
      "loss": 0.3726,
      "step": 9000
    },
    {
      "epoch": 1.049375897492544,
      "grad_norm": 4.393484592437744,
      "learning_rate": 3.2512242718804084e-05,
      "loss": 0.3388,
      "step": 9500
    },
    {
      "epoch": 1.1046062078868883,
      "grad_norm": 5.16934871673584,
      "learning_rate": 3.159173754556501e-05,
      "loss": 0.3257,
      "step": 10000
    },
    {
      "epoch": 1.1598365182812327,
      "grad_norm": 5.631825923919678,
      "learning_rate": 3.0671232372325935e-05,
      "loss": 0.3317,
      "step": 10500
    },
    {
      "epoch": 1.2150668286755772,
      "grad_norm": 8.464137077331543,
      "learning_rate": 2.9750727199086858e-05,
      "loss": 0.3355,
      "step": 11000
    },
    {
      "epoch": 1.2702971390699216,
      "grad_norm": 3.7784786224365234,
      "learning_rate": 2.8830222025847787e-05,
      "loss": 0.3237,
      "step": 11500
    },
    {
      "epoch": 1.325527449464266,
      "grad_norm": 3.2196037769317627,
      "learning_rate": 2.7909716852608713e-05,
      "loss": 0.3337,
      "step": 12000
    },
    {
      "epoch": 1.3807577598586103,
      "grad_norm": 3.235119342803955,
      "learning_rate": 2.6989211679369642e-05,
      "loss": 0.3361,
      "step": 12500
    },
    {
      "epoch": 1.4359880702529548,
      "grad_norm": 4.776684284210205,
      "learning_rate": 2.6068706506130565e-05,
      "loss": 0.3293,
      "step": 13000
    },
    {
      "epoch": 1.4912183806472992,
      "grad_norm": 2.502559185028076,
      "learning_rate": 2.514820133289149e-05,
      "loss": 0.3306,
      "step": 13500
    },
    {
      "epoch": 1.5464486910416437,
      "grad_norm": 3.863410472869873,
      "learning_rate": 2.422769615965242e-05,
      "loss": 0.3263,
      "step": 14000
    },
    {
      "epoch": 1.601679001435988,
      "grad_norm": 5.39610481262207,
      "learning_rate": 2.3307190986413342e-05,
      "loss": 0.3264,
      "step": 14500
    },
    {
      "epoch": 1.6569093118303324,
      "grad_norm": 3.842740774154663,
      "learning_rate": 2.238668581317427e-05,
      "loss": 0.3256,
      "step": 15000
    },
    {
      "epoch": 1.7121396222246767,
      "grad_norm": 5.5820159912109375,
      "learning_rate": 2.1466180639935197e-05,
      "loss": 0.3198,
      "step": 15500
    },
    {
      "epoch": 1.7673699326190213,
      "grad_norm": 4.859297752380371,
      "learning_rate": 2.0545675466696123e-05,
      "loss": 0.3177,
      "step": 16000
    },
    {
      "epoch": 1.8226002430133659,
      "grad_norm": 3.055142879486084,
      "learning_rate": 1.9625170293457052e-05,
      "loss": 0.323,
      "step": 16500
    },
    {
      "epoch": 1.8778305534077102,
      "grad_norm": 4.110532283782959,
      "learning_rate": 1.8704665120217975e-05,
      "loss": 0.3215,
      "step": 17000
    },
    {
      "epoch": 1.9330608638020546,
      "grad_norm": 3.1046953201293945,
      "learning_rate": 1.7784159946978904e-05,
      "loss": 0.3184,
      "step": 17500
    },
    {
      "epoch": 1.988291174196399,
      "grad_norm": 4.723021507263184,
      "learning_rate": 1.686365477373983e-05,
      "loss": 0.3098,
      "step": 18000
    },
    {
      "epoch": 2.0435214845907432,
      "grad_norm": 3.669624090194702,
      "learning_rate": 1.5943149600500756e-05,
      "loss": 0.2955,
      "step": 18500
    },
    {
      "epoch": 2.098751794985088,
      "grad_norm": 4.306643486022949,
      "learning_rate": 1.5022644427261682e-05,
      "loss": 0.2917,
      "step": 19000
    },
    {
      "epoch": 2.1539821053794324,
      "grad_norm": 5.152012825012207,
      "learning_rate": 1.410213925402261e-05,
      "loss": 0.2937,
      "step": 19500
    },
    {
      "epoch": 2.2092124157737767,
      "grad_norm": 3.9263572692871094,
      "learning_rate": 1.3181634080783534e-05,
      "loss": 0.2856,
      "step": 20000
    },
    {
      "epoch": 2.264442726168121,
      "grad_norm": 5.799157619476318,
      "learning_rate": 1.2261128907544461e-05,
      "loss": 0.2841,
      "step": 20500
    },
    {
      "epoch": 2.3196730365624654,
      "grad_norm": 4.7339982986450195,
      "learning_rate": 1.1340623734305387e-05,
      "loss": 0.286,
      "step": 21000
    },
    {
      "epoch": 2.3749033469568097,
      "grad_norm": 3.6318821907043457,
      "learning_rate": 1.0420118561066314e-05,
      "loss": 0.281,
      "step": 21500
    },
    {
      "epoch": 2.4301336573511545,
      "grad_norm": 5.960201740264893,
      "learning_rate": 9.49961338782724e-06,
      "loss": 0.2792,
      "step": 22000
    },
    {
      "epoch": 2.485363967745499,
      "grad_norm": 3.6307778358459473,
      "learning_rate": 8.579108214588166e-06,
      "loss": 0.2823,
      "step": 22500
    },
    {
      "epoch": 2.540594278139843,
      "grad_norm": 4.094478607177734,
      "learning_rate": 7.658603041349092e-06,
      "loss": 0.2833,
      "step": 23000
    },
    {
      "epoch": 2.5958245885341875,
      "grad_norm": 6.529913425445557,
      "learning_rate": 6.738097868110019e-06,
      "loss": 0.2828,
      "step": 23500
    },
    {
      "epoch": 2.651054898928532,
      "grad_norm": 4.902674674987793,
      "learning_rate": 5.8175926948709455e-06,
      "loss": 0.2765,
      "step": 24000
    },
    {
      "epoch": 2.7062852093228766,
      "grad_norm": 3.3057799339294434,
      "learning_rate": 4.897087521631872e-06,
      "loss": 0.2797,
      "step": 24500
    },
    {
      "epoch": 2.7615155197172205,
      "grad_norm": 2.9364888668060303,
      "learning_rate": 3.976582348392798e-06,
      "loss": 0.2775,
      "step": 25000
    },
    {
      "epoch": 2.8167458301115653,
      "grad_norm": 2.8349802494049072,
      "learning_rate": 3.0560771751537244e-06,
      "loss": 0.2772,
      "step": 25500
    },
    {
      "epoch": 2.8719761405059097,
      "grad_norm": 5.067656517028809,
      "learning_rate": 2.135572001914651e-06,
      "loss": 0.2788,
      "step": 26000
    },
    {
      "epoch": 2.927206450900254,
      "grad_norm": 5.148288726806641,
      "learning_rate": 1.2150668286755771e-06,
      "loss": 0.2762,
      "step": 26500
    },
    {
      "epoch": 2.9824367612945983,
      "grad_norm": 4.261458873748779,
      "learning_rate": 2.945616554365036e-07,
      "loss": 0.2857,
      "step": 27000
    }
  ],
  "logging_steps": 500,
  "max_steps": 27159,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2783919846857984e+16,
  "train_batch_size": 64,
  "trial_name": null,
  "trial_params": null
}
