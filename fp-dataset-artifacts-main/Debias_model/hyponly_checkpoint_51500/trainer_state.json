{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.999767008387698,
  "eval_steps": 500,
  "global_step": 51500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.029123951537744643,
      "grad_norm": 3.9260435104370117,
      "learning_rate": 4.951557160608885e-05,
      "loss": 0.9853,
      "step": 500
    },
    {
      "epoch": 0.058247903075489285,
      "grad_norm": 3.2456607818603516,
      "learning_rate": 4.9030172413793105e-05,
      "loss": 0.8827,
      "step": 1000
    },
    {
      "epoch": 0.08737185461323392,
      "grad_norm": 4.220388889312744,
      "learning_rate": 4.854477322149736e-05,
      "loss": 0.8448,
      "step": 1500
    },
    {
      "epoch": 0.11649580615097857,
      "grad_norm": 3.384363889694214,
      "learning_rate": 4.805937402920162e-05,
      "loss": 0.8252,
      "step": 2000
    },
    {
      "epoch": 0.1456197576887232,
      "grad_norm": 5.5078582763671875,
      "learning_rate": 4.757397483690587e-05,
      "loss": 0.8188,
      "step": 2500
    },
    {
      "epoch": 0.17474370922646784,
      "grad_norm": 3.835218906402588,
      "learning_rate": 4.7088575644610125e-05,
      "loss": 0.8082,
      "step": 3000
    },
    {
      "epoch": 0.20386766076421248,
      "grad_norm": 4.489749908447266,
      "learning_rate": 4.660317645231438e-05,
      "loss": 0.8025,
      "step": 3500
    },
    {
      "epoch": 0.23299161230195714,
      "grad_norm": 4.17827844619751,
      "learning_rate": 4.611777726001864e-05,
      "loss": 0.7947,
      "step": 4000
    },
    {
      "epoch": 0.26211556383970175,
      "grad_norm": 4.182633876800537,
      "learning_rate": 4.56323780677229e-05,
      "loss": 0.7944,
      "step": 4500
    },
    {
      "epoch": 0.2912395153774464,
      "grad_norm": 3.7970566749572754,
      "learning_rate": 4.514697887542715e-05,
      "loss": 0.7955,
      "step": 5000
    },
    {
      "epoch": 0.3203634669151911,
      "grad_norm": 4.513530731201172,
      "learning_rate": 4.466157968313141e-05,
      "loss": 0.7864,
      "step": 5500
    },
    {
      "epoch": 0.3494874184529357,
      "grad_norm": 3.4864771366119385,
      "learning_rate": 4.417618049083567e-05,
      "loss": 0.7827,
      "step": 6000
    },
    {
      "epoch": 0.37861136999068035,
      "grad_norm": 3.2703356742858887,
      "learning_rate": 4.369078129853993e-05,
      "loss": 0.7832,
      "step": 6500
    },
    {
      "epoch": 0.40773532152842495,
      "grad_norm": 3.1146209239959717,
      "learning_rate": 4.320538210624418e-05,
      "loss": 0.7739,
      "step": 7000
    },
    {
      "epoch": 0.4368592730661696,
      "grad_norm": 2.6282958984375,
      "learning_rate": 4.271998291394843e-05,
      "loss": 0.7815,
      "step": 7500
    },
    {
      "epoch": 0.4659832246039143,
      "grad_norm": 3.725196123123169,
      "learning_rate": 4.223458372165269e-05,
      "loss": 0.769,
      "step": 8000
    },
    {
      "epoch": 0.4951071761416589,
      "grad_norm": 4.249058246612549,
      "learning_rate": 4.1749184529356947e-05,
      "loss": 0.7713,
      "step": 8500
    },
    {
      "epoch": 0.5242311276794035,
      "grad_norm": 3.540884494781494,
      "learning_rate": 4.12637853370612e-05,
      "loss": 0.7764,
      "step": 9000
    },
    {
      "epoch": 0.5533550792171482,
      "grad_norm": 3.381821393966675,
      "learning_rate": 4.0778386144765457e-05,
      "loss": 0.7651,
      "step": 9500
    },
    {
      "epoch": 0.5824790307548928,
      "grad_norm": 3.519137382507324,
      "learning_rate": 4.0292986952469715e-05,
      "loss": 0.7678,
      "step": 10000
    },
    {
      "epoch": 0.6116029822926374,
      "grad_norm": 2.9670450687408447,
      "learning_rate": 3.980758776017397e-05,
      "loss": 0.7656,
      "step": 10500
    },
    {
      "epoch": 0.6407269338303822,
      "grad_norm": 3.564852476119995,
      "learning_rate": 3.9322188567878225e-05,
      "loss": 0.7619,
      "step": 11000
    },
    {
      "epoch": 0.6698508853681268,
      "grad_norm": 3.3889236450195312,
      "learning_rate": 3.883678937558248e-05,
      "loss": 0.7687,
      "step": 11500
    },
    {
      "epoch": 0.6989748369058714,
      "grad_norm": 4.901176452636719,
      "learning_rate": 3.835139018328674e-05,
      "loss": 0.7633,
      "step": 12000
    },
    {
      "epoch": 0.7280987884436161,
      "grad_norm": 3.774479627609253,
      "learning_rate": 3.786599099099099e-05,
      "loss": 0.7581,
      "step": 12500
    },
    {
      "epoch": 0.7572227399813607,
      "grad_norm": 3.2273178100585938,
      "learning_rate": 3.7380591798695245e-05,
      "loss": 0.7561,
      "step": 13000
    },
    {
      "epoch": 0.7863466915191053,
      "grad_norm": 3.7644128799438477,
      "learning_rate": 3.68951926063995e-05,
      "loss": 0.7532,
      "step": 13500
    },
    {
      "epoch": 0.8154706430568499,
      "grad_norm": 3.8686254024505615,
      "learning_rate": 3.640979341410376e-05,
      "loss": 0.7504,
      "step": 14000
    },
    {
      "epoch": 0.8445945945945946,
      "grad_norm": 4.680526256561279,
      "learning_rate": 3.592439422180802e-05,
      "loss": 0.7469,
      "step": 14500
    },
    {
      "epoch": 0.8737185461323392,
      "grad_norm": 3.5285208225250244,
      "learning_rate": 3.543899502951227e-05,
      "loss": 0.7529,
      "step": 15000
    },
    {
      "epoch": 0.9028424976700838,
      "grad_norm": 3.132964611053467,
      "learning_rate": 3.495359583721653e-05,
      "loss": 0.7542,
      "step": 15500
    },
    {
      "epoch": 0.9319664492078286,
      "grad_norm": 2.5644278526306152,
      "learning_rate": 3.446819664492079e-05,
      "loss": 0.7582,
      "step": 16000
    },
    {
      "epoch": 0.9610904007455732,
      "grad_norm": 3.552117347717285,
      "learning_rate": 3.398279745262504e-05,
      "loss": 0.7524,
      "step": 16500
    },
    {
      "epoch": 0.9902143522833178,
      "grad_norm": 3.11271595954895,
      "learning_rate": 3.34973982603293e-05,
      "loss": 0.7332,
      "step": 17000
    },
    {
      "epoch": 1.0193383038210624,
      "grad_norm": 4.525396347045898,
      "learning_rate": 3.301199906803355e-05,
      "loss": 0.7253,
      "step": 17500
    },
    {
      "epoch": 1.048462255358807,
      "grad_norm": 3.081141471862793,
      "learning_rate": 3.252659987573781e-05,
      "loss": 0.7281,
      "step": 18000
    },
    {
      "epoch": 1.0775862068965518,
      "grad_norm": 5.492474555969238,
      "learning_rate": 3.204120068344206e-05,
      "loss": 0.7099,
      "step": 18500
    },
    {
      "epoch": 1.1067101584342964,
      "grad_norm": 2.7875969409942627,
      "learning_rate": 3.155580149114632e-05,
      "loss": 0.7263,
      "step": 19000
    },
    {
      "epoch": 1.135834109972041,
      "grad_norm": 3.386772871017456,
      "learning_rate": 3.1070402298850576e-05,
      "loss": 0.7184,
      "step": 19500
    },
    {
      "epoch": 1.1649580615097856,
      "grad_norm": 3.9927799701690674,
      "learning_rate": 3.0585003106554835e-05,
      "loss": 0.7247,
      "step": 20000
    },
    {
      "epoch": 1.1940820130475303,
      "grad_norm": 3.6873726844787598,
      "learning_rate": 3.009960391425909e-05,
      "loss": 0.7281,
      "step": 20500
    },
    {
      "epoch": 1.2232059645852749,
      "grad_norm": 3.7874810695648193,
      "learning_rate": 2.9614204721963345e-05,
      "loss": 0.7199,
      "step": 21000
    },
    {
      "epoch": 1.2523299161230197,
      "grad_norm": 4.995805740356445,
      "learning_rate": 2.9128805529667603e-05,
      "loss": 0.7179,
      "step": 21500
    },
    {
      "epoch": 1.281453867660764,
      "grad_norm": 3.5166103839874268,
      "learning_rate": 2.8643406337371858e-05,
      "loss": 0.7132,
      "step": 22000
    },
    {
      "epoch": 1.310577819198509,
      "grad_norm": 3.596029043197632,
      "learning_rate": 2.815800714507611e-05,
      "loss": 0.721,
      "step": 22500
    },
    {
      "epoch": 1.3397017707362535,
      "grad_norm": 4.256988048553467,
      "learning_rate": 2.7672607952780365e-05,
      "loss": 0.7155,
      "step": 23000
    },
    {
      "epoch": 1.3688257222739981,
      "grad_norm": 3.690355062484741,
      "learning_rate": 2.7187208760484623e-05,
      "loss": 0.7164,
      "step": 23500
    },
    {
      "epoch": 1.3979496738117427,
      "grad_norm": 4.591859817504883,
      "learning_rate": 2.6701809568188878e-05,
      "loss": 0.7225,
      "step": 24000
    },
    {
      "epoch": 1.4270736253494873,
      "grad_norm": 4.3793721199035645,
      "learning_rate": 2.6216410375893136e-05,
      "loss": 0.7176,
      "step": 24500
    },
    {
      "epoch": 1.4561975768872322,
      "grad_norm": 5.02000093460083,
      "learning_rate": 2.573101118359739e-05,
      "loss": 0.7142,
      "step": 25000
    },
    {
      "epoch": 1.4853215284249768,
      "grad_norm": 3.904676675796509,
      "learning_rate": 2.524561199130165e-05,
      "loss": 0.71,
      "step": 25500
    },
    {
      "epoch": 1.5144454799627214,
      "grad_norm": 3.668257236480713,
      "learning_rate": 2.47602127990059e-05,
      "loss": 0.715,
      "step": 26000
    },
    {
      "epoch": 1.543569431500466,
      "grad_norm": 3.1633963584899902,
      "learning_rate": 2.427481360671016e-05,
      "loss": 0.717,
      "step": 26500
    },
    {
      "epoch": 1.5726933830382106,
      "grad_norm": 3.321945905685425,
      "learning_rate": 2.3789414414414415e-05,
      "loss": 0.7156,
      "step": 27000
    },
    {
      "epoch": 1.6018173345759554,
      "grad_norm": 4.001171588897705,
      "learning_rate": 2.3304015222118673e-05,
      "loss": 0.7244,
      "step": 27500
    },
    {
      "epoch": 1.6309412861136998,
      "grad_norm": 4.272627353668213,
      "learning_rate": 2.2818616029822928e-05,
      "loss": 0.7151,
      "step": 28000
    },
    {
      "epoch": 1.6600652376514446,
      "grad_norm": 3.1134746074676514,
      "learning_rate": 2.2333216837527183e-05,
      "loss": 0.7132,
      "step": 28500
    },
    {
      "epoch": 1.689189189189189,
      "grad_norm": 3.8299036026000977,
      "learning_rate": 2.1847817645231438e-05,
      "loss": 0.7065,
      "step": 29000
    },
    {
      "epoch": 1.7183131407269339,
      "grad_norm": 2.914578437805176,
      "learning_rate": 2.1362418452935696e-05,
      "loss": 0.7162,
      "step": 29500
    },
    {
      "epoch": 1.7474370922646785,
      "grad_norm": 4.156020641326904,
      "learning_rate": 2.087701926063995e-05,
      "loss": 0.711,
      "step": 30000
    },
    {
      "epoch": 1.776561043802423,
      "grad_norm": 3.7169511318206787,
      "learning_rate": 2.039162006834421e-05,
      "loss": 0.7175,
      "step": 30500
    },
    {
      "epoch": 1.805684995340168,
      "grad_norm": 3.5535035133361816,
      "learning_rate": 1.990622087604846e-05,
      "loss": 0.7033,
      "step": 31000
    },
    {
      "epoch": 1.8348089468779123,
      "grad_norm": 4.221675395965576,
      "learning_rate": 1.942082168375272e-05,
      "loss": 0.7215,
      "step": 31500
    },
    {
      "epoch": 1.8639328984156571,
      "grad_norm": 3.003340721130371,
      "learning_rate": 1.8935422491456975e-05,
      "loss": 0.7072,
      "step": 32000
    },
    {
      "epoch": 1.8930568499534017,
      "grad_norm": 3.043015956878662,
      "learning_rate": 1.8450023299161233e-05,
      "loss": 0.7173,
      "step": 32500
    },
    {
      "epoch": 1.9221808014911463,
      "grad_norm": 3.1277573108673096,
      "learning_rate": 1.7964624106865488e-05,
      "loss": 0.7177,
      "step": 33000
    },
    {
      "epoch": 1.951304753028891,
      "grad_norm": 4.054072856903076,
      "learning_rate": 1.7479224914569743e-05,
      "loss": 0.71,
      "step": 33500
    },
    {
      "epoch": 1.9804287045666356,
      "grad_norm": 3.2354941368103027,
      "learning_rate": 1.6993825722273998e-05,
      "loss": 0.7104,
      "step": 34000
    },
    {
      "epoch": 2.0095526561043804,
      "grad_norm": 4.8897271156311035,
      "learning_rate": 1.6508426529978256e-05,
      "loss": 0.6904,
      "step": 34500
    },
    {
      "epoch": 2.0386766076421248,
      "grad_norm": 6.52296781539917,
      "learning_rate": 1.602302733768251e-05,
      "loss": 0.6747,
      "step": 35000
    },
    {
      "epoch": 2.0678005591798696,
      "grad_norm": 3.057589054107666,
      "learning_rate": 1.5537628145386766e-05,
      "loss": 0.681,
      "step": 35500
    },
    {
      "epoch": 2.096924510717614,
      "grad_norm": 3.8155806064605713,
      "learning_rate": 1.5052228953091021e-05,
      "loss": 0.6714,
      "step": 36000
    },
    {
      "epoch": 2.126048462255359,
      "grad_norm": 4.566387176513672,
      "learning_rate": 1.4566829760795278e-05,
      "loss": 0.6893,
      "step": 36500
    },
    {
      "epoch": 2.1551724137931036,
      "grad_norm": 5.270562171936035,
      "learning_rate": 1.4081430568499535e-05,
      "loss": 0.6792,
      "step": 37000
    },
    {
      "epoch": 2.184296365330848,
      "grad_norm": 4.460037708282471,
      "learning_rate": 1.3596031376203791e-05,
      "loss": 0.6887,
      "step": 37500
    },
    {
      "epoch": 2.213420316868593,
      "grad_norm": 3.8572006225585938,
      "learning_rate": 1.3110632183908048e-05,
      "loss": 0.6762,
      "step": 38000
    },
    {
      "epoch": 2.2425442684063372,
      "grad_norm": 3.591357946395874,
      "learning_rate": 1.2625232991612301e-05,
      "loss": 0.6788,
      "step": 38500
    },
    {
      "epoch": 2.271668219944082,
      "grad_norm": 4.230599403381348,
      "learning_rate": 1.2139833799316558e-05,
      "loss": 0.6805,
      "step": 39000
    },
    {
      "epoch": 2.3007921714818265,
      "grad_norm": 4.472224235534668,
      "learning_rate": 1.1654434607020815e-05,
      "loss": 0.686,
      "step": 39500
    },
    {
      "epoch": 2.3299161230195713,
      "grad_norm": 3.5422587394714355,
      "learning_rate": 1.1169035414725071e-05,
      "loss": 0.688,
      "step": 40000
    },
    {
      "epoch": 2.359040074557316,
      "grad_norm": 4.229339599609375,
      "learning_rate": 1.0683636222429326e-05,
      "loss": 0.6777,
      "step": 40500
    },
    {
      "epoch": 2.3881640260950605,
      "grad_norm": 4.338491439819336,
      "learning_rate": 1.0198237030133583e-05,
      "loss": 0.6671,
      "step": 41000
    },
    {
      "epoch": 2.4172879776328053,
      "grad_norm": 4.616970539093018,
      "learning_rate": 9.712837837837838e-06,
      "loss": 0.6711,
      "step": 41500
    },
    {
      "epoch": 2.4464119291705497,
      "grad_norm": 4.728475570678711,
      "learning_rate": 9.227438645542095e-06,
      "loss": 0.6833,
      "step": 42000
    },
    {
      "epoch": 2.4755358807082946,
      "grad_norm": 3.930331230163574,
      "learning_rate": 8.742039453246351e-06,
      "loss": 0.6644,
      "step": 42500
    },
    {
      "epoch": 2.5046598322460394,
      "grad_norm": 4.438766956329346,
      "learning_rate": 8.256640260950606e-06,
      "loss": 0.6766,
      "step": 43000
    },
    {
      "epoch": 2.5337837837837838,
      "grad_norm": 4.259390354156494,
      "learning_rate": 7.771241068654863e-06,
      "loss": 0.6695,
      "step": 43500
    },
    {
      "epoch": 2.562907735321528,
      "grad_norm": 4.35086727142334,
      "learning_rate": 7.285841876359117e-06,
      "loss": 0.6827,
      "step": 44000
    },
    {
      "epoch": 2.592031686859273,
      "grad_norm": 4.63039493560791,
      "learning_rate": 6.800442684063374e-06,
      "loss": 0.6715,
      "step": 44500
    },
    {
      "epoch": 2.621155638397018,
      "grad_norm": 4.074797630310059,
      "learning_rate": 6.31504349176763e-06,
      "loss": 0.6724,
      "step": 45000
    },
    {
      "epoch": 2.650279589934762,
      "grad_norm": 3.2345545291900635,
      "learning_rate": 5.829644299471886e-06,
      "loss": 0.6835,
      "step": 45500
    },
    {
      "epoch": 2.679403541472507,
      "grad_norm": 4.0923871994018555,
      "learning_rate": 5.344245107176142e-06,
      "loss": 0.6691,
      "step": 46000
    },
    {
      "epoch": 2.7085274930102514,
      "grad_norm": 6.196164131164551,
      "learning_rate": 4.858845914880398e-06,
      "loss": 0.6779,
      "step": 46500
    },
    {
      "epoch": 2.7376514445479962,
      "grad_norm": 4.277214050292969,
      "learning_rate": 4.373446722584654e-06,
      "loss": 0.6709,
      "step": 47000
    },
    {
      "epoch": 2.766775396085741,
      "grad_norm": 4.485130310058594,
      "learning_rate": 3.8880475302889095e-06,
      "loss": 0.6743,
      "step": 47500
    },
    {
      "epoch": 2.7958993476234855,
      "grad_norm": 3.911250114440918,
      "learning_rate": 3.402648337993166e-06,
      "loss": 0.6749,
      "step": 48000
    },
    {
      "epoch": 2.8250232991612303,
      "grad_norm": 3.560828447341919,
      "learning_rate": 2.917249145697422e-06,
      "loss": 0.6743,
      "step": 48500
    },
    {
      "epoch": 2.8541472506989747,
      "grad_norm": 2.8529186248779297,
      "learning_rate": 2.431849953401678e-06,
      "loss": 0.674,
      "step": 49000
    },
    {
      "epoch": 2.8832712022367195,
      "grad_norm": 3.836865186691284,
      "learning_rate": 1.9464507611059337e-06,
      "loss": 0.6769,
      "step": 49500
    },
    {
      "epoch": 2.9123951537744643,
      "grad_norm": 4.194027423858643,
      "learning_rate": 1.4610515688101895e-06,
      "loss": 0.686,
      "step": 50000
    },
    {
      "epoch": 2.9415191053122087,
      "grad_norm": 4.9458818435668945,
      "learning_rate": 9.756523765144455e-07,
      "loss": 0.6821,
      "step": 50500
    },
    {
      "epoch": 2.9706430568499536,
      "grad_norm": 4.2889533042907715,
      "learning_rate": 4.902531842187015e-07,
      "loss": 0.6729,
      "step": 51000
    },
    {
      "epoch": 2.999767008387698,
      "grad_norm": 4.74727201461792,
      "learning_rate": 4.85399192295744e-09,
      "loss": 0.6739,
      "step": 51500
    }
  ],
  "logging_steps": 500,
  "max_steps": 51504,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.2121084906495488e+16,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
