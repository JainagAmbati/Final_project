{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 51504,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.029123951537744643,
      "grad_norm": 7.190219879150391,
      "learning_rate": 4.951557160608885e-05,
      "loss": 0.7412,
      "step": 500
    },
    {
      "epoch": 0.058247903075489285,
      "grad_norm": 6.545936107635498,
      "learning_rate": 4.9030172413793105e-05,
      "loss": 0.6108,
      "step": 1000
    },
    {
      "epoch": 0.08737185461323392,
      "grad_norm": 4.269355297088623,
      "learning_rate": 4.854477322149736e-05,
      "loss": 0.5672,
      "step": 1500
    },
    {
      "epoch": 0.11649580615097857,
      "grad_norm": 5.616466522216797,
      "learning_rate": 4.805937402920162e-05,
      "loss": 0.5579,
      "step": 2000
    },
    {
      "epoch": 0.1456197576887232,
      "grad_norm": 3.407534599304199,
      "learning_rate": 4.757397483690587e-05,
      "loss": 0.5335,
      "step": 2500
    },
    {
      "epoch": 0.17474370922646784,
      "grad_norm": 3.8647730350494385,
      "learning_rate": 4.7088575644610125e-05,
      "loss": 0.5247,
      "step": 3000
    },
    {
      "epoch": 0.20386766076421248,
      "grad_norm": 6.987254619598389,
      "learning_rate": 4.660317645231438e-05,
      "loss": 0.5176,
      "step": 3500
    },
    {
      "epoch": 0.23299161230195714,
      "grad_norm": 5.682243347167969,
      "learning_rate": 4.611777726001864e-05,
      "loss": 0.5112,
      "step": 4000
    },
    {
      "epoch": 0.26211556383970175,
      "grad_norm": 6.136110782623291,
      "learning_rate": 4.56323780677229e-05,
      "loss": 0.5012,
      "step": 4500
    },
    {
      "epoch": 0.2912395153774464,
      "grad_norm": 6.837510585784912,
      "learning_rate": 4.514697887542715e-05,
      "loss": 0.5,
      "step": 5000
    },
    {
      "epoch": 0.3203634669151911,
      "grad_norm": 5.585024833679199,
      "learning_rate": 4.466157968313141e-05,
      "loss": 0.5,
      "step": 5500
    },
    {
      "epoch": 0.3494874184529357,
      "grad_norm": 3.337376832962036,
      "learning_rate": 4.417618049083567e-05,
      "loss": 0.4894,
      "step": 6000
    },
    {
      "epoch": 0.37861136999068035,
      "grad_norm": 5.368521213531494,
      "learning_rate": 4.369078129853993e-05,
      "loss": 0.4799,
      "step": 6500
    },
    {
      "epoch": 0.40773532152842495,
      "grad_norm": 6.597304344177246,
      "learning_rate": 4.320538210624418e-05,
      "loss": 0.4862,
      "step": 7000
    },
    {
      "epoch": 0.4368592730661696,
      "grad_norm": 3.882396936416626,
      "learning_rate": 4.271998291394843e-05,
      "loss": 0.4857,
      "step": 7500
    },
    {
      "epoch": 0.4659832246039143,
      "grad_norm": 4.94938850402832,
      "learning_rate": 4.223458372165269e-05,
      "loss": 0.4849,
      "step": 8000
    },
    {
      "epoch": 0.4951071761416589,
      "grad_norm": 8.797968864440918,
      "learning_rate": 4.1749184529356947e-05,
      "loss": 0.4698,
      "step": 8500
    },
    {
      "epoch": 0.5242311276794035,
      "grad_norm": 3.7954983711242676,
      "learning_rate": 4.12637853370612e-05,
      "loss": 0.4677,
      "step": 9000
    },
    {
      "epoch": 0.5533550792171482,
      "grad_norm": 3.8643929958343506,
      "learning_rate": 4.0778386144765457e-05,
      "loss": 0.4771,
      "step": 9500
    },
    {
      "epoch": 0.5824790307548928,
      "grad_norm": 2.715282678604126,
      "learning_rate": 4.0292986952469715e-05,
      "loss": 0.4783,
      "step": 10000
    },
    {
      "epoch": 0.6116029822926374,
      "grad_norm": 2.7500216960906982,
      "learning_rate": 3.980758776017397e-05,
      "loss": 0.4681,
      "step": 10500
    },
    {
      "epoch": 0.6407269338303822,
      "grad_norm": 3.8511874675750732,
      "learning_rate": 3.9322188567878225e-05,
      "loss": 0.466,
      "step": 11000
    },
    {
      "epoch": 0.6698508853681268,
      "grad_norm": 3.6825971603393555,
      "learning_rate": 3.883678937558248e-05,
      "loss": 0.4653,
      "step": 11500
    },
    {
      "epoch": 0.6989748369058714,
      "grad_norm": 4.174781799316406,
      "learning_rate": 3.835139018328674e-05,
      "loss": 0.4575,
      "step": 12000
    },
    {
      "epoch": 0.7280987884436161,
      "grad_norm": 6.065310955047607,
      "learning_rate": 3.786599099099099e-05,
      "loss": 0.463,
      "step": 12500
    },
    {
      "epoch": 0.7572227399813607,
      "grad_norm": 3.084970712661743,
      "learning_rate": 3.7380591798695245e-05,
      "loss": 0.4598,
      "step": 13000
    },
    {
      "epoch": 0.7863466915191053,
      "grad_norm": 7.0095744132995605,
      "learning_rate": 3.68951926063995e-05,
      "loss": 0.4524,
      "step": 13500
    },
    {
      "epoch": 0.8154706430568499,
      "grad_norm": 2.5492942333221436,
      "learning_rate": 3.640979341410376e-05,
      "loss": 0.4476,
      "step": 14000
    },
    {
      "epoch": 0.8445945945945946,
      "grad_norm": 5.481369495391846,
      "learning_rate": 3.592439422180802e-05,
      "loss": 0.4511,
      "step": 14500
    },
    {
      "epoch": 0.8737185461323392,
      "grad_norm": 3.6424965858459473,
      "learning_rate": 3.543899502951227e-05,
      "loss": 0.4566,
      "step": 15000
    },
    {
      "epoch": 0.9028424976700838,
      "grad_norm": 3.516481637954712,
      "learning_rate": 3.495359583721653e-05,
      "loss": 0.4504,
      "step": 15500
    },
    {
      "epoch": 0.9319664492078286,
      "grad_norm": 4.248123645782471,
      "learning_rate": 3.446819664492079e-05,
      "loss": 0.4528,
      "step": 16000
    },
    {
      "epoch": 0.9610904007455732,
      "grad_norm": 5.523550987243652,
      "learning_rate": 3.398279745262504e-05,
      "loss": 0.4493,
      "step": 16500
    },
    {
      "epoch": 0.9902143522833178,
      "grad_norm": 4.800368309020996,
      "learning_rate": 3.34973982603293e-05,
      "loss": 0.4445,
      "step": 17000
    },
    {
      "epoch": 1.0193383038210624,
      "grad_norm": 4.302993297576904,
      "learning_rate": 3.301199906803355e-05,
      "loss": 0.4301,
      "step": 17500
    },
    {
      "epoch": 1.048462255358807,
      "grad_norm": 1.6163980960845947,
      "learning_rate": 3.252659987573781e-05,
      "loss": 0.4187,
      "step": 18000
    },
    {
      "epoch": 1.0775862068965518,
      "grad_norm": 7.195991516113281,
      "learning_rate": 3.204120068344206e-05,
      "loss": 0.4118,
      "step": 18500
    },
    {
      "epoch": 1.1067101584342964,
      "grad_norm": 3.925314426422119,
      "learning_rate": 3.155580149114632e-05,
      "loss": 0.4276,
      "step": 19000
    },
    {
      "epoch": 1.135834109972041,
      "grad_norm": 3.689948558807373,
      "learning_rate": 3.1070402298850576e-05,
      "loss": 0.4192,
      "step": 19500
    },
    {
      "epoch": 1.1649580615097856,
      "grad_norm": 6.157018184661865,
      "learning_rate": 3.0585003106554835e-05,
      "loss": 0.4307,
      "step": 20000
    },
    {
      "epoch": 1.1940820130475303,
      "grad_norm": 7.114372253417969,
      "learning_rate": 3.009960391425909e-05,
      "loss": 0.4266,
      "step": 20500
    },
    {
      "epoch": 1.2232059645852749,
      "grad_norm": 3.0019967555999756,
      "learning_rate": 2.9614204721963345e-05,
      "loss": 0.4176,
      "step": 21000
    },
    {
      "epoch": 1.2523299161230197,
      "grad_norm": 5.575146675109863,
      "learning_rate": 2.9128805529667603e-05,
      "loss": 0.4124,
      "step": 21500
    },
    {
      "epoch": 1.281453867660764,
      "grad_norm": 10.592133522033691,
      "learning_rate": 2.8643406337371858e-05,
      "loss": 0.41,
      "step": 22000
    },
    {
      "epoch": 1.310577819198509,
      "grad_norm": 4.521232604980469,
      "learning_rate": 2.815800714507611e-05,
      "loss": 0.4217,
      "step": 22500
    },
    {
      "epoch": 1.3397017707362535,
      "grad_norm": 3.136889696121216,
      "learning_rate": 2.7672607952780365e-05,
      "loss": 0.418,
      "step": 23000
    },
    {
      "epoch": 1.3688257222739981,
      "grad_norm": 1.28839111328125,
      "learning_rate": 2.7187208760484623e-05,
      "loss": 0.416,
      "step": 23500
    },
    {
      "epoch": 1.3979496738117427,
      "grad_norm": 9.929242134094238,
      "learning_rate": 2.6701809568188878e-05,
      "loss": 0.4145,
      "step": 24000
    },
    {
      "epoch": 1.4270736253494873,
      "grad_norm": 4.130100727081299,
      "learning_rate": 2.6216410375893136e-05,
      "loss": 0.4132,
      "step": 24500
    },
    {
      "epoch": 1.4561975768872322,
      "grad_norm": 7.753206253051758,
      "learning_rate": 2.573101118359739e-05,
      "loss": 0.4115,
      "step": 25000
    },
    {
      "epoch": 1.4853215284249768,
      "grad_norm": 8.83862590789795,
      "learning_rate": 2.524561199130165e-05,
      "loss": 0.4072,
      "step": 25500
    },
    {
      "epoch": 1.5144454799627214,
      "grad_norm": 4.370875835418701,
      "learning_rate": 2.47602127990059e-05,
      "loss": 0.4188,
      "step": 26000
    },
    {
      "epoch": 1.543569431500466,
      "grad_norm": 1.9658421277999878,
      "learning_rate": 2.427481360671016e-05,
      "loss": 0.4123,
      "step": 26500
    },
    {
      "epoch": 1.5726933830382106,
      "grad_norm": 5.728204250335693,
      "learning_rate": 2.3789414414414415e-05,
      "loss": 0.4108,
      "step": 27000
    },
    {
      "epoch": 1.6018173345759554,
      "grad_norm": 6.561862468719482,
      "learning_rate": 2.3304015222118673e-05,
      "loss": 0.4156,
      "step": 27500
    },
    {
      "epoch": 1.6309412861136998,
      "grad_norm": 5.049428462982178,
      "learning_rate": 2.2818616029822928e-05,
      "loss": 0.4157,
      "step": 28000
    },
    {
      "epoch": 1.6600652376514446,
      "grad_norm": 8.247108459472656,
      "learning_rate": 2.2333216837527183e-05,
      "loss": 0.4074,
      "step": 28500
    },
    {
      "epoch": 1.689189189189189,
      "grad_norm": 5.210123538970947,
      "learning_rate": 2.1847817645231438e-05,
      "loss": 0.4047,
      "step": 29000
    },
    {
      "epoch": 1.7183131407269339,
      "grad_norm": 4.662685394287109,
      "learning_rate": 2.1362418452935696e-05,
      "loss": 0.4068,
      "step": 29500
    },
    {
      "epoch": 1.7474370922646785,
      "grad_norm": 4.111356258392334,
      "learning_rate": 2.087701926063995e-05,
      "loss": 0.4101,
      "step": 30000
    },
    {
      "epoch": 1.776561043802423,
      "grad_norm": 3.2359166145324707,
      "learning_rate": 2.039162006834421e-05,
      "loss": 0.4143,
      "step": 30500
    },
    {
      "epoch": 1.805684995340168,
      "grad_norm": 7.091109752655029,
      "learning_rate": 1.990622087604846e-05,
      "loss": 0.4023,
      "step": 31000
    },
    {
      "epoch": 1.8348089468779123,
      "grad_norm": 3.7990505695343018,
      "learning_rate": 1.942082168375272e-05,
      "loss": 0.4023,
      "step": 31500
    },
    {
      "epoch": 1.8639328984156571,
      "grad_norm": 4.107101917266846,
      "learning_rate": 1.8935422491456975e-05,
      "loss": 0.404,
      "step": 32000
    },
    {
      "epoch": 1.8930568499534017,
      "grad_norm": 2.7158126831054688,
      "learning_rate": 1.8450023299161233e-05,
      "loss": 0.4084,
      "step": 32500
    },
    {
      "epoch": 1.9221808014911463,
      "grad_norm": 3.091386556625366,
      "learning_rate": 1.7964624106865488e-05,
      "loss": 0.4001,
      "step": 33000
    },
    {
      "epoch": 1.951304753028891,
      "grad_norm": 4.816603660583496,
      "learning_rate": 1.7479224914569743e-05,
      "loss": 0.3984,
      "step": 33500
    },
    {
      "epoch": 1.9804287045666356,
      "grad_norm": 3.9889745712280273,
      "learning_rate": 1.6993825722273998e-05,
      "loss": 0.4103,
      "step": 34000
    },
    {
      "epoch": 2.0095526561043804,
      "grad_norm": 5.415433883666992,
      "learning_rate": 1.6508426529978256e-05,
      "loss": 0.39,
      "step": 34500
    },
    {
      "epoch": 2.0386766076421248,
      "grad_norm": 5.740991115570068,
      "learning_rate": 1.602302733768251e-05,
      "loss": 0.3775,
      "step": 35000
    },
    {
      "epoch": 2.0678005591798696,
      "grad_norm": 3.0810492038726807,
      "learning_rate": 1.5537628145386766e-05,
      "loss": 0.3716,
      "step": 35500
    },
    {
      "epoch": 2.096924510717614,
      "grad_norm": 3.9777488708496094,
      "learning_rate": 1.5052228953091021e-05,
      "loss": 0.368,
      "step": 36000
    },
    {
      "epoch": 2.126048462255359,
      "grad_norm": 1.3741915225982666,
      "learning_rate": 1.4566829760795278e-05,
      "loss": 0.3803,
      "step": 36500
    },
    {
      "epoch": 2.1551724137931036,
      "grad_norm": 3.818351984024048,
      "learning_rate": 1.4081430568499535e-05,
      "loss": 0.3806,
      "step": 37000
    },
    {
      "epoch": 2.184296365330848,
      "grad_norm": 3.625837564468384,
      "learning_rate": 1.3596031376203791e-05,
      "loss": 0.3793,
      "step": 37500
    },
    {
      "epoch": 2.213420316868593,
      "grad_norm": 2.826514959335327,
      "learning_rate": 1.3110632183908048e-05,
      "loss": 0.375,
      "step": 38000
    },
    {
      "epoch": 2.2425442684063372,
      "grad_norm": 3.195235013961792,
      "learning_rate": 1.2625232991612301e-05,
      "loss": 0.3827,
      "step": 38500
    },
    {
      "epoch": 2.271668219944082,
      "grad_norm": 4.144399642944336,
      "learning_rate": 1.2139833799316558e-05,
      "loss": 0.3738,
      "step": 39000
    },
    {
      "epoch": 2.3007921714818265,
      "grad_norm": 2.161651134490967,
      "learning_rate": 1.1654434607020815e-05,
      "loss": 0.3752,
      "step": 39500
    },
    {
      "epoch": 2.3299161230195713,
      "grad_norm": 7.359389305114746,
      "learning_rate": 1.1169035414725071e-05,
      "loss": 0.3789,
      "step": 40000
    },
    {
      "epoch": 2.359040074557316,
      "grad_norm": 5.897207736968994,
      "learning_rate": 1.0683636222429326e-05,
      "loss": 0.3744,
      "step": 40500
    },
    {
      "epoch": 2.3881640260950605,
      "grad_norm": 3.0749588012695312,
      "learning_rate": 1.0198237030133583e-05,
      "loss": 0.3758,
      "step": 41000
    },
    {
      "epoch": 2.4172879776328053,
      "grad_norm": 6.118377685546875,
      "learning_rate": 9.712837837837838e-06,
      "loss": 0.3713,
      "step": 41500
    },
    {
      "epoch": 2.4464119291705497,
      "grad_norm": 8.920466423034668,
      "learning_rate": 9.227438645542095e-06,
      "loss": 0.3767,
      "step": 42000
    },
    {
      "epoch": 2.4755358807082946,
      "grad_norm": 3.6063761711120605,
      "learning_rate": 8.742039453246351e-06,
      "loss": 0.3729,
      "step": 42500
    },
    {
      "epoch": 2.5046598322460394,
      "grad_norm": 2.2086849212646484,
      "learning_rate": 8.256640260950606e-06,
      "loss": 0.3705,
      "step": 43000
    },
    {
      "epoch": 2.5337837837837838,
      "grad_norm": 5.198488712310791,
      "learning_rate": 7.771241068654863e-06,
      "loss": 0.3763,
      "step": 43500
    },
    {
      "epoch": 2.562907735321528,
      "grad_norm": 2.9538633823394775,
      "learning_rate": 7.285841876359117e-06,
      "loss": 0.3752,
      "step": 44000
    },
    {
      "epoch": 2.592031686859273,
      "grad_norm": 3.8829903602600098,
      "learning_rate": 6.800442684063374e-06,
      "loss": 0.372,
      "step": 44500
    },
    {
      "epoch": 2.621155638397018,
      "grad_norm": 4.967466831207275,
      "learning_rate": 6.31504349176763e-06,
      "loss": 0.3727,
      "step": 45000
    },
    {
      "epoch": 2.650279589934762,
      "grad_norm": 4.153894424438477,
      "learning_rate": 5.829644299471886e-06,
      "loss": 0.3752,
      "step": 45500
    },
    {
      "epoch": 2.679403541472507,
      "grad_norm": 5.102790355682373,
      "learning_rate": 5.344245107176142e-06,
      "loss": 0.3744,
      "step": 46000
    },
    {
      "epoch": 2.7085274930102514,
      "grad_norm": 6.013782501220703,
      "learning_rate": 4.858845914880398e-06,
      "loss": 0.3676,
      "step": 46500
    },
    {
      "epoch": 2.7376514445479962,
      "grad_norm": 6.520234107971191,
      "learning_rate": 4.373446722584654e-06,
      "loss": 0.3673,
      "step": 47000
    },
    {
      "epoch": 2.766775396085741,
      "grad_norm": 6.1536030769348145,
      "learning_rate": 3.8880475302889095e-06,
      "loss": 0.3696,
      "step": 47500
    },
    {
      "epoch": 2.7958993476234855,
      "grad_norm": 5.994361877441406,
      "learning_rate": 3.402648337993166e-06,
      "loss": 0.3769,
      "step": 48000
    },
    {
      "epoch": 2.8250232991612303,
      "grad_norm": 3.1723880767822266,
      "learning_rate": 2.917249145697422e-06,
      "loss": 0.3753,
      "step": 48500
    },
    {
      "epoch": 2.8541472506989747,
      "grad_norm": 3.39389967918396,
      "learning_rate": 2.431849953401678e-06,
      "loss": 0.3781,
      "step": 49000
    },
    {
      "epoch": 2.8832712022367195,
      "grad_norm": 5.205613613128662,
      "learning_rate": 1.9464507611059337e-06,
      "loss": 0.3736,
      "step": 49500
    },
    {
      "epoch": 2.9123951537744643,
      "grad_norm": 2.1917643547058105,
      "learning_rate": 1.4610515688101895e-06,
      "loss": 0.3781,
      "step": 50000
    },
    {
      "epoch": 2.9415191053122087,
      "grad_norm": 4.420770168304443,
      "learning_rate": 9.756523765144455e-07,
      "loss": 0.3777,
      "step": 50500
    },
    {
      "epoch": 2.9706430568499536,
      "grad_norm": 8.001749992370605,
      "learning_rate": 4.902531842187015e-07,
      "loss": 0.3696,
      "step": 51000
    },
    {
      "epoch": 2.999767008387698,
      "grad_norm": 6.812790393829346,
      "learning_rate": 4.85399192295744e-09,
      "loss": 0.3711,
      "step": 51500
    }
  ],
  "logging_steps": 500,
  "max_steps": 51504,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
